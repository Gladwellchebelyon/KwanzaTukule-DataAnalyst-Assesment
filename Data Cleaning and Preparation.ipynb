{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "file_path = \"Case Study Data - Read Only - case_study_data_2025-01-16T06_49_12.19881Z.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333405 entries, 0 to 333404\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   DATE                 333405 non-null  object\n",
      " 1   ANONYMIZED CATEGORY  333405 non-null  object\n",
      " 2   ANONYMIZED PRODUCT   333405 non-null  object\n",
      " 3   ANONYMIZED BUSINESS  333405 non-null  object\n",
      " 4   ANONYMIZED LOCATION  333405 non-null  object\n",
      " 5   QUANTITY             333405 non-null  int64 \n",
      " 6   VALUE                333397 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 17.8+ MB\n",
      "None\n",
      "(333405, 7)\n",
      "                       DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "0  August 18, 2024, 9:32 PM        Category-106       Product-21f4   \n",
      "1  August 18, 2024, 9:32 PM        Category-120       Product-4156   \n",
      "2  August 18, 2024, 9:32 PM        Category-121       Product-49bd   \n",
      "3  August 18, 2024, 9:32 PM         Category-76       Product-61dd   \n",
      "4  August 18, 2024, 9:32 PM        Category-119       Product-66e0   \n",
      "\n",
      "  ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "0       Business-de42       Location-1ba8         1    850  \n",
      "1       Business-de42       Location-1ba8         2  1,910  \n",
      "2       Business-de42       Location-1ba8         1  3,670  \n",
      "3       Business-de42       Location-1ba8         1  2,605  \n",
      "4       Business-de42       Location-1ba8         5  1,480  \n",
      "            QUANTITY\n",
      "count  333405.000000\n",
      "mean        2.321186\n",
      "std         3.790614\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       359.000000\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Quality Report ===\n",
      "\n",
      "Dataset Shape: (333405, 7) (rows, columns)\n",
      "\n",
      "1. Missing Values:\n",
      "       Missing Count  Missing Percentage\n",
      "VALUE              8                 0.0\n",
      "\n",
      "2. Duplicate Rows:\n",
      "Total duplicates: 3524 (1.06%)\n",
      "\n",
      "3. Numerical Columns Summary:\n",
      "            QUANTITY\n",
      "count  333405.000000\n",
      "mean        2.321186\n",
      "std         3.790614\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       359.000000\n",
      "\n",
      "4. Categorical Columns Summary:\n",
      "\n",
      "DATE:\n",
      "- Unique values: 96703\n",
      "\n",
      "ANONYMIZED CATEGORY:\n",
      "- Unique values: 46\n",
      "\n",
      "ANONYMIZED PRODUCT:\n",
      "- Unique values: 820\n",
      "\n",
      "ANONYMIZED BUSINESS:\n",
      "- Unique values: 4800\n",
      "\n",
      "ANONYMIZED LOCATION:\n",
      "- Unique values: 53\n",
      "\n",
      "VALUE:\n",
      "- Unique values: 1050\n",
      "3. Outliers (using IQR method):\n",
      "QUANTITY: 48631 outliers detected\n",
      "\n",
      "\n",
      "6. Class Distribution (for categorical columns):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_data_types(df):\n",
    "    print(\"\\n=== Column Data Types Analysis ===\")\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    print(\"\\nNumerical Columns:\")\n",
    "    for col in numeric_cols:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "    print(\"\\nCategorical Columns:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "def check_data_quality(df):\n",
    "    print(\"\\n=== Data Quality Report ===\")\n",
    "    \n",
    "    # Get total rows and columns\n",
    "    print(f\"\\nDataset Shape: {df.shape} (rows, columns)\")\n",
    "    \n",
    "    # 1. Missing Values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing Percentage': missing_percent.round(2)\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Count'] > 0])\n",
    "    \n",
    "    # 2. Duplicates\n",
    "    print(\"\\n2. Duplicate Rows:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Total duplicates: {duplicates} ({(duplicates/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Basic Statistics for Numerical Columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\n3. Numerical Columns Summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # 4. Unique Values in Categorical Columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\n4. Categorical Columns Summary:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"- Unique values: {unique_count}\")\n",
    "            if unique_count < 10:  # Only show value counts if fewer than 10 unique values\n",
    "                print(\"- Value counts:\")\n",
    "                print(df[col].value_counts())\n",
    "    # 5. Outliers (for numerical columns)\n",
    "    print(\"3. Outliers (using IQR method):\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "        print(f\"{col}: {len(outliers)} outliers detected\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "     # 6. Class Imbalance (if target variable is specified)\n",
    "    print(\"6. Class Distribution (for categorical columns):\")\n",
    "    for col in categorical_cols:\n",
    "        balance = df[col].value_counts(normalize=True)\n",
    "        if len(balance) < 10:  # Only show if fewer than 10 unique values\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(balance.multiply(100).round(2).astype(str) + '%')\n",
    "            print(\"\\n\")\n",
    "    \n",
    "check_data_quality(df)\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# numeric_cols, categorical_cols = analyze_data_types(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Values Analysis ===\n",
      "\n",
      "Before cleaning:\n",
      "Missing values in each column:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    print(\"\\n=== Missing Values Analysis ===\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Convert VALUE to numeric, handling the commas\n",
    "    df_temp['VALUE'] = df_temp['VALUE'].replace({',': ''}, regex=True)\n",
    "    \n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(df_temp.isnull().sum())\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Run it\n",
    "df_temp = check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Duplicates Analysis ===\n",
      "\n",
      "Total number of duplicate rows: 3524\n",
      "\n",
      "First few duplicate rows:\n",
      "                            DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "6153   January 6, 2024, 11:52 AM         Category-91       Product-1b48   \n",
      "7554       July 9, 2024, 2:26 PM        Category-104       Product-af50   \n",
      "7555       July 9, 2024, 2:26 PM         Category-92       Product-d09a   \n",
      "12238    April 19, 2024, 3:19 PM         Category-75       Product-086d   \n",
      "12239    April 19, 2024, 3:19 PM        Category-106       Product-21f4   \n",
      "\n",
      "      ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "6153        Business-20fc       Location-b125         1  3,680  \n",
      "7554        Business-476c       Location-b27b         1  1,310  \n",
      "7555        Business-476c       Location-b27b         1  1,550  \n",
      "12238       Business-b48e       Location-03fc         3  2,090  \n",
      "12239       Business-b48e       Location-03fc         2    850  \n",
      "\n",
      "Duplicate counts by column combinations:\n",
      "DATE: 236702 duplicates\n",
      "ANONYMIZED CATEGORY: 333359 duplicates\n",
      "ANONYMIZED PRODUCT: 332585 duplicates\n",
      "ANONYMIZED BUSINESS: 328605 duplicates\n",
      "ANONYMIZED LOCATION: 333352 duplicates\n",
      "QUANTITY: 333326 duplicates\n",
      "VALUE: 332354 duplicates\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(df):\n",
    "    print(\"\\n=== Duplicates Analysis ===\")\n",
    "    \n",
    "    # Count total duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"\\nTotal number of duplicate rows: {dup_count}\")\n",
    "    \n",
    "    # Show example of duplicates\n",
    "    if dup_count > 0:\n",
    "        print(\"\\nFirst few duplicate rows:\")\n",
    "        duplicates = df[df.duplicated(keep='first')]\n",
    "        print(duplicates.head())\n",
    "        \n",
    "        # Check which columns are most commonly duplicated\n",
    "        print(\"\\nDuplicate counts by column combinations:\")\n",
    "        for col in df.columns:\n",
    "            dup_by_col = df.duplicated(subset=[col]).sum()\n",
    "            print(f\"{col}: {dup_by_col} duplicates\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Run it\n",
    "duplicates = check_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUANTITY Outliers Analysis ===\n",
      "\n",
      "QUANTITY statistics:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "Outlier boundaries:\n",
      "Lower bound: -0.5\n",
      "Upper bound: 3.5\n",
      "\n",
      "Number of outliers: 48631\n",
      "\n",
      "Outlier QUANTITY values:\n",
      "QUANTITY\n",
      "5     18511\n",
      "4      9834\n",
      "10     8232\n",
      "6      3262\n",
      "7      1648\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_outliers(df):\n",
    "    print(\"\\n=== QUANTITY Outliers Analysis ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nQUANTITY statistics:\")\n",
    "    print(df['QUANTITY'].describe())\n",
    "    \n",
    "    # Calculate outlier boundaries\n",
    "    Q1 = df['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"\\nOutlier boundaries:\")\n",
    "    print(f\"Lower bound: {lower_bound}\")\n",
    "    print(f\"Upper bound: {upper_bound}\")\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = df[(df['QUANTITY'] < lower_bound) | (df['QUANTITY'] > upper_bound)]\n",
    "    print(f\"\\nNumber of outliers: {len(outliers)}\")\n",
    "    \n",
    "    # Show distribution of outlier quantities\n",
    "    print(\"\\nOutlier QUANTITY values:\")\n",
    "    print(outliers['QUANTITY'].value_counts().head())\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Run it\n",
    "outliers = check_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning process...\n",
      "\n",
      "=== Missing Values Analysis ===\n",
      "\n",
      "Before cleaning:\n",
      "Missing values in each column:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n",
      "\n",
      "=== Duplicates Analysis ===\n",
      "\n",
      "Total number of duplicate rows: 3524\n",
      "\n",
      "First few duplicate rows:\n",
      "                            DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "6153   January 6, 2024, 11:52 AM         Category-91       Product-1b48   \n",
      "7554       July 9, 2024, 2:26 PM        Category-104       Product-af50   \n",
      "7555       July 9, 2024, 2:26 PM         Category-92       Product-d09a   \n",
      "12238    April 19, 2024, 3:19 PM         Category-75       Product-086d   \n",
      "12239    April 19, 2024, 3:19 PM        Category-106       Product-21f4   \n",
      "\n",
      "      ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "6153        Business-20fc       Location-b125         1  3,680  \n",
      "7554        Business-476c       Location-b27b         1  1,310  \n",
      "7555        Business-476c       Location-b27b         1  1,550  \n",
      "12238       Business-b48e       Location-03fc         3  2,090  \n",
      "12239       Business-b48e       Location-03fc         2    850  \n",
      "\n",
      "Duplicate counts by column combinations:\n",
      "DATE: 236702 duplicates\n",
      "ANONYMIZED CATEGORY: 333359 duplicates\n",
      "ANONYMIZED PRODUCT: 332585 duplicates\n",
      "ANONYMIZED BUSINESS: 328605 duplicates\n",
      "ANONYMIZED LOCATION: 333352 duplicates\n",
      "QUANTITY: 333326 duplicates\n",
      "VALUE: 332354 duplicates\n",
      "\n",
      "=== QUANTITY Outliers Analysis ===\n",
      "\n",
      "QUANTITY statistics:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "Outlier boundaries:\n",
      "Lower bound: -0.5\n",
      "Upper bound: 3.5\n",
      "\n",
      "Number of outliers: 48631\n",
      "\n",
      "Outlier QUANTITY values:\n",
      "QUANTITY\n",
      "5     18511\n",
      "4      9834\n",
      "10     8232\n",
      "6      3262\n",
      "7      1648\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Cleaning Data Step by Step ===\n",
      "\n",
      "Step 1: Cleaning VALUE column\n",
      "VALUE column cleaned\n",
      "\n",
      "Step 2: Removing duplicates\n",
      "Removed 3524 duplicate rows\n",
      "\n",
      "Step 3: Flagging outliers\n",
      "Flagged 48115 outliers\n",
      "\n",
      "Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "def clean_data_step_by_step(df):\n",
    "    print(\"\\n=== Cleaning Data Step by Step ===\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Clean VALUE column\n",
    "    print(\"\\nStep 1: Cleaning VALUE column\")\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].replace({',': ''}, regex=True)\n",
    "    print(\"VALUE column cleaned\")\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    print(\"\\nStep 2: Removing duplicates\")\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    rows_removed = initial_rows - len(df_clean)\n",
    "    print(f\"Removed {rows_removed} duplicate rows\")\n",
    "    \n",
    "    # 3. Flag outliers\n",
    "    print(\"\\nStep 3: Flagging outliers\")\n",
    "    Q1 = df_clean['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df_clean['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_clean['is_outlier'] = ((df_clean['QUANTITY'] < lower_bound) | \n",
    "                             (df_clean['QUANTITY'] > upper_bound))\n",
    "    \n",
    "    outlier_count = df_clean['is_outlier'].sum()\n",
    "    print(f\"Flagged {outlier_count} outliers\")\n",
    "    \n",
    "    print(\"\\nCleaning complete!\")\n",
    "    return df_clean\n",
    "\n",
    "# Run the complete analysis\n",
    "print(\"Starting data cleaning process...\")\n",
    "df_temp = check_missing_values(df)\n",
    "duplicates = check_duplicates(df)\n",
    "outliers = check_outliers(df)\n",
    "df_clean = clean_data_step_by_step(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Verification Report ===\n",
      "\n",
      "1. Row Count Comparison:\n",
      "Original rows: 333405\n",
      "Cleaned rows: 329881\n",
      "Rows removed: 3524\n",
      "\n",
      "2. Missing Values Check:\n",
      "\n",
      "Before cleaning:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "is_outlier             0\n",
      "dtype: int64\n",
      "\n",
      "3. Duplicates Check:\n",
      "Original duplicates: 3524\n",
      "Remaining duplicates: 0\n",
      "\n",
      "4. QUANTITY Statistics Comparison:\n",
      "\n",
      "Before cleaning:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "After cleaning:\n",
      "count    329881.000000\n",
      "mean          2.321507\n",
      "std           3.767796\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "5. Outlier Flags:\n",
      "Number of rows flagged as outliers: 48115\n",
      "Percentage of outliers: 14.59%\n",
      "\n",
      "6. VALUE Column Format:\n",
      "\n",
      "Before cleaning (first 5 unique values):\n",
      "['850' '1,910' '3,670' '2,605' '1,480']\n",
      "\n",
      "After cleaning (first 5 unique values):\n",
      "['850' '1910' '3670' '2605' '1480']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verify_cleaning(df_original, df_cleaned):\n",
    "    print(\"\\n=== Cleaning Verification Report ===\")\n",
    "    \n",
    "    # 1. Compare row counts\n",
    "    print(\"\\n1. Row Count Comparison:\")\n",
    "    print(f\"Original rows: {len(df_original)}\")\n",
    "    print(f\"Cleaned rows: {len(df_cleaned)}\")\n",
    "    print(f\"Rows removed: {len(df_original) - len(df_cleaned)}\")\n",
    "    \n",
    "    # 2. Check if missing values were handled\n",
    "    print(\"\\n2. Missing Values Check:\")\n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(df_original.isnull().sum())\n",
    "    print(\"\\nAfter cleaning:\")\n",
    "    print(df_cleaned.isnull().sum())\n",
    "    \n",
    "    # 3. Check for duplicates\n",
    "    print(\"\\n3. Duplicates Check:\")\n",
    "    print(f\"Original duplicates: {df_original.duplicated().sum()}\")\n",
    "    print(f\"Remaining duplicates: {df_cleaned.duplicated().sum()}\")\n",
    "    \n",
    "    # 4. Compare QUANTITY statistics\n",
    "    print(\"\\n4. QUANTITY Statistics Comparison:\")\n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(df_original['QUANTITY'].describe())\n",
    "    print(\"\\nAfter cleaning:\")\n",
    "    print(df_cleaned['QUANTITY'].describe())\n",
    "    \n",
    "    # 5. Check outlier flags\n",
    "    if 'is_outlier' in df_cleaned.columns:\n",
    "        print(\"\\n5. Outlier Flags:\")\n",
    "        outlier_count = df_cleaned['is_outlier'].sum()\n",
    "        print(f\"Number of rows flagged as outliers: {outlier_count}\")\n",
    "        print(f\"Percentage of outliers: {(outlier_count/len(df_cleaned))*100:.2f}%\")\n",
    "    \n",
    "    # 6. Check VALUE column format\n",
    "    print(\"\\n6. VALUE Column Format:\")\n",
    "    print(\"\\nBefore cleaning (first 5 unique values):\")\n",
    "    print(df_original['VALUE'].unique()[:5])\n",
    "    print(\"\\nAfter cleaning (first 5 unique values):\")\n",
    "    print(df_cleaned['VALUE'].unique()[:5])\n",
    "\n",
    "# Run the verification\n",
    "verify_cleaning(df, df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Cleaning Steps ===\n",
      "Converting VALUE to numeric...\n",
      "Handling missing values...\n",
      "Removing duplicates...\n",
      "\n",
      "Cleaning complete! Let's verify the results...\n",
      "\n",
      "Verification:\n",
      "Missing values remaining: 0\n",
      "VALUE dtype: float64\n",
      "Sample of VALUE column (first 5):\n",
      "0     850.0\n",
      "1    1910.0\n",
      "2    3670.0\n",
      "3    2605.0\n",
      "4    1480.0\n",
      "Name: VALUE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def final_cleaning(df):\n",
    "    print(\"\\n=== Final Cleaning Steps ===\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Properly convert VALUE to numeric\n",
    "    print(\"Converting VALUE to numeric...\")\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].str.replace(',', '').astype(float)\n",
    "    \n",
    "    # 2. Handle missing values in VALUE\n",
    "    print(\"Handling missing values...\")\n",
    "    value_median = df_clean['VALUE'].median()\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].fillna(value_median)\n",
    "    \n",
    "    # 3. Remove duplicates (already done but included for completeness)\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    # 4. Flag outliers (already done but included for completeness)\n",
    "    Q1 = df_clean['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df_clean['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_clean['is_outlier'] = ((df_clean['QUANTITY'] < lower_bound) | \n",
    "                             (df_clean['QUANTITY'] > upper_bound))\n",
    "    \n",
    "    print(\"\\nCleaning complete! Let's verify the results...\")\n",
    "    \n",
    "    # Quick verification\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"Missing values remaining: {df_clean.isnull().sum().sum()}\")\n",
    "    print(f\"VALUE dtype: {df_clean['VALUE'].dtype}\")\n",
    "    print(f\"Sample of VALUE column (first 5):\")\n",
    "    print(df_clean['VALUE'].head())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Run the final cleaning\n",
    "df_final = final_cleaning(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Clean Data Summary ===\n",
      "\n",
      "1. Dataset Shape:\n",
      "Rows: 329881\n",
      "Columns: 8\n",
      "\n",
      "2. Data Types:\n",
      "DATE                    object\n",
      "ANONYMIZED CATEGORY     object\n",
      "ANONYMIZED PRODUCT      object\n",
      "ANONYMIZED BUSINESS     object\n",
      "ANONYMIZED LOCATION     object\n",
      "QUANTITY                 int64\n",
      "VALUE                  float64\n",
      "is_outlier                bool\n",
      "dtype: object\n",
      "\n",
      "3. Value Ranges:\n",
      "\n",
      "QUANTITY:\n",
      "count    329881.000000\n",
      "mean          2.321507\n",
      "std           3.767796\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "VALUE:\n",
      "count    329881.000000\n",
      "mean       2319.004962\n",
      "std        1582.561268\n",
      "min           0.000000\n",
      "25%        1420.000000\n",
      "50%        1840.000000\n",
      "75%        2750.000000\n",
      "max       16136.000000\n",
      "Name: VALUE, dtype: float64\n",
      "\n",
      "4. Outlier Distribution:\n",
      "Normal transactions: 281766\n",
      "Outlier transactions: 48115\n"
     ]
    }
   ],
   "source": [
    "def summarize_clean_data(df):\n",
    "    print(\"\\n=== Clean Data Summary ===\")\n",
    "    \n",
    "    print(\"\\n1. Dataset Shape:\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n3. Value Ranges:\")\n",
    "    numeric_cols = ['QUANTITY', 'VALUE']\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].describe())\n",
    "    \n",
    "    print(\"\\n4. Outlier Distribution:\")\n",
    "    print(\"Normal transactions:\", len(df[~df['is_outlier']]))\n",
    "    print(\"Outlier transactions:\", len(df[df['is_outlier']]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run summary\n",
    "df_summary = summarize_clean_data(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE ENGINEERING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering process...\n",
      "\n",
      "=== Creating Time-Based Features ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BENNLANGAT\\AppData\\Local\\Temp\\ipykernel_8992\\2066435689.py:8: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['DATE'] = pd.to_datetime(df['DATE'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created time features: year, month, day, day_of_week, is_weekend, quarter\n",
      "\n",
      "=== Creating Transaction Features ===\n",
      "Created features: price_per_unit, transaction_size, order_size\n",
      "\n",
      "=== Creating Business Features ===\n",
      "Created features: business_frequency, business_avg_value\n",
      "\n",
      "=== Creating Product Features ===\n",
      "Created features: product_popularity, product_avg_price\n",
      "\n",
      "=== Creating Location Features ===\n",
      "Created features: location_frequency, location_avg_value\n",
      "\n",
      "=== New Features Summary ===\n",
      "\n",
      "New columns added:\n",
      "['business_avg_value', 'business_frequency', 'day', 'day_of_week', 'is_weekend', 'location_avg_value', 'location_frequency', 'month', 'order_size', 'price_per_unit', 'product_avg_price', 'product_popularity', 'quarter', 'transaction_size', 'year']\n",
      "\n",
      "Dataset shape:\n",
      "Original: (329881, 8)\n",
      "After feature engineering: (329881, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_time_features(df):\n",
    "    print(\"\\n=== Creating Time-Based Features ===\")\n",
    "    \n",
    "    # Convert DATE to datetime if not already\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # Extract time components\n",
    "    df['year'] = df['DATE'].dt.year\n",
    "    df['month'] = df['DATE'].dt.month\n",
    "    df['day'] = df['DATE'].dt.day\n",
    "    df['day_of_week'] = df['DATE'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['quarter'] = df['DATE'].dt.quarter\n",
    "    \n",
    "    print(\"Created time features: year, month, day, day_of_week, is_weekend, quarter\")\n",
    "    return df\n",
    "\n",
    "def create_transaction_features(df):\n",
    "    print(\"\\n=== Creating Transaction Features ===\")\n",
    "    \n",
    "    # Price per unit\n",
    "    df['price_per_unit'] = df['VALUE'] / df['QUANTITY']\n",
    "    \n",
    "    # Transaction size categories\n",
    "    df['transaction_size'] = pd.qcut(df['VALUE'], q=5, labels=['Very Small', 'Small', 'Medium', 'Large', 'Very Large'])\n",
    "    \n",
    "    # Order size categories\n",
    "    df['order_size'] = pd.qcut(df['QUANTITY'], q=3, labels=['Small', 'Medium', 'Large'])\n",
    "    \n",
    "    print(\"Created features: price_per_unit, transaction_size, order_size\")\n",
    "    return df\n",
    "\n",
    "def create_business_features(df):\n",
    "    print(\"\\n=== Creating Business Features ===\")\n",
    "    \n",
    "    # Business frequency\n",
    "    business_freq = df.groupby('ANONYMIZED BUSINESS').size()\n",
    "    df['business_frequency'] = df['ANONYMIZED BUSINESS'].map(business_freq)\n",
    "    \n",
    "    # Business average transaction value\n",
    "    business_avg_value = df.groupby('ANONYMIZED BUSINESS')['VALUE'].mean()\n",
    "    df['business_avg_value'] = df['ANONYMIZED BUSINESS'].map(business_avg_value)\n",
    "    \n",
    "    print(\"Created features: business_frequency, business_avg_value\")\n",
    "    return df\n",
    "\n",
    "def create_product_features(df):\n",
    "    print(\"\\n=== Creating Product Features ===\")\n",
    "    \n",
    "    # Product popularity\n",
    "    product_freq = df.groupby('ANONYMIZED PRODUCT').size()\n",
    "    df['product_popularity'] = df['ANONYMIZED PRODUCT'].map(product_freq)\n",
    "    \n",
    "    # Product average price\n",
    "    product_avg_price = df.groupby('ANONYMIZED PRODUCT')['price_per_unit'].mean()\n",
    "    df['product_avg_price'] = df['ANONYMIZED PRODUCT'].map(product_avg_price)\n",
    "    \n",
    "    print(\"Created features: product_popularity, product_avg_price\")\n",
    "    return df\n",
    "\n",
    "def create_location_features(df):\n",
    "    print(\"\\n=== Creating Location Features ===\")\n",
    "    \n",
    "    # Location transaction frequency\n",
    "    location_freq = df.groupby('ANONYMIZED LOCATION').size()\n",
    "    df['location_frequency'] = df['ANONYMIZED LOCATION'].map(location_freq)\n",
    "    \n",
    "    # Location average transaction value\n",
    "    location_avg_value = df.groupby('ANONYMIZED LOCATION')['VALUE'].mean()\n",
    "    df['location_avg_value'] = df['ANONYMIZED LOCATION'].map(location_avg_value)\n",
    "    \n",
    "    print(\"Created features: location_frequency, location_avg_value\")\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    print(\"Starting feature engineering process...\")\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Apply all feature engineering steps\n",
    "    df_featured = create_time_features(df_featured)\n",
    "    df_featured = create_transaction_features(df_featured)\n",
    "    df_featured = create_business_features(df_featured)\n",
    "    df_featured = create_product_features(df_featured)\n",
    "    df_featured = create_location_features(df_featured)\n",
    "    \n",
    "    # Show new features summary\n",
    "    print(\"\\n=== New Features Summary ===\")\n",
    "    print(\"\\nNew columns added:\")\n",
    "    new_columns = set(df_featured.columns) - set(df.columns)\n",
    "    print(sorted(list(new_columns)))\n",
    "    \n",
    "    print(\"\\nDataset shape:\")\n",
    "    print(f\"Original: {df.shape}\")\n",
    "    print(f\"After feature engineering: {df_featured.shape}\")\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "# Run feature engineering\n",
    "df_with_features = feature_engineering(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== New Features Analysis ===\n",
      "\n",
      "Transactions by day of week:\n",
      "             count         mean\n",
      "day_of_week                    \n",
      "0            58037  2275.572859\n",
      "1            52028  2362.002076\n",
      "2            50666  2319.359097\n",
      "3            54499  2348.988018\n",
      "4            55990  2318.728076\n",
      "5             2315  2242.573218\n",
      "6            56346  2298.135129\n",
      "\n",
      "Weekday vs Weekend transactions:\n",
      "             count         mean\n",
      "is_weekend                     \n",
      "0           271220  2323.993057\n",
      "1            58661  2295.942432\n",
      "\n",
      "Transaction size distribution:\n",
      "transaction_size\n",
      "Large         0.204177\n",
      "Small         0.203734\n",
      "Very Small    0.200027\n",
      "Medium        0.196556\n",
      "Very Large    0.195507\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Price per unit statistics:\n",
      "count    329881.0\n",
      "mean          inf\n",
      "std           NaN\n",
      "min           0.0\n",
      "25%         657.5\n",
      "50%        1380.0\n",
      "75%        2130.0\n",
      "max           inf\n",
      "Name: price_per_unit, dtype: float64\n",
      "\n",
      "Top 5 locations by frequency:\n",
      "88979     43891\n",
      "67792     43891\n",
      "255860    43891\n",
      "255859    43891\n",
      "310437    43891\n",
      "Name: location_frequency, dtype: int64\n",
      "\n",
      "Top 5 most popular products:\n",
      "195386    24736\n",
      "115409    24736\n",
      "144761    24736\n",
      "295618    24736\n",
      "215639    24736\n",
      "Name: product_popularity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def analyze_new_features(df):\n",
    "    print(\"\\n=== New Features Analysis ===\")\n",
    "    \n",
    "    # 1. Time-based patterns\n",
    "    print(\"\\nTransactions by day of week:\")\n",
    "    print(df.groupby('day_of_week')['VALUE'].agg(['count', 'mean']))\n",
    "    \n",
    "    print(\"\\nWeekday vs Weekend transactions:\")\n",
    "    print(df.groupby('is_weekend')['VALUE'].agg(['count', 'mean']))\n",
    "    \n",
    "    # 2. Transaction size distribution\n",
    "    print(\"\\nTransaction size distribution:\")\n",
    "    print(df['transaction_size'].value_counts(normalize=True))\n",
    "    \n",
    "    # 3. Price per unit statistics\n",
    "    print(\"\\nPrice per unit statistics:\")\n",
    "    print(df['price_per_unit'].describe())\n",
    "    \n",
    "    # 4. Location analysis\n",
    "    print(\"\\nTop 5 locations by frequency:\")\n",
    "    print(df['location_frequency'].sort_values(ascending=False).head())\n",
    "    \n",
    "    # 5. Product popularity\n",
    "    print(\"\\nTop 5 most popular products:\")\n",
    "    print(df['product_popularity'].sort_values(ascending=False).head())\n",
    "\n",
    "# Run analysis\n",
    "analyze_new_features(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dates from my dataset:\n",
      "0   2024-08-18 21:32:00\n",
      "1   2024-08-18 21:32:00\n",
      "2   2024-08-18 21:32:00\n",
      "3   2024-08-18 21:32:00\n",
      "4   2024-08-18 21:32:00\n",
      "Name: DATE, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Check the first few dates to see their format\n",
    "print(\"Sample dates from my dataset:\")\n",
    "print(df_final['DATE'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Month-Year Feature ===\n",
      "\n",
      "Sample of the new Month-Year feature:\n",
      "                 DATE   Month-Year\n",
      "0 2024-08-18 21:32:00  August 2024\n",
      "1 2024-08-18 21:32:00  August 2024\n",
      "2 2024-08-18 21:32:00  August 2024\n",
      "3 2024-08-18 21:32:00  August 2024\n",
      "4 2024-08-18 21:32:00  August 2024\n",
      "5 2024-08-18 21:32:00  August 2024\n",
      "6 2024-08-18 21:32:00  August 2024\n",
      "7 2024-08-18 21:32:00  August 2024\n",
      "8 2024-08-18 21:32:00  August 2024\n",
      "9 2024-08-18 21:32:00  August 2024\n",
      "\n",
      "Unique Month-Year values in the dataset:\n",
      "['April 2024', 'August 2024', 'December 2024', 'February 2024', 'January 2024', 'July 2024', 'June 2024', 'March 2024', 'May 2024', 'November 2024', 'October 2024', 'September 2024']\n"
     ]
    }
   ],
   "source": [
    "def create_month_year_feature(df):\n",
    "    print(\"\\n=== Creating Month-Year Feature ===\")\n",
    "    \n",
    "    # Create Month-Year column (no need to convert DATE as it's already datetime)\n",
    "    df['Month-Year'] = df['DATE'].dt.strftime('%B %Y')\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nSample of the new Month-Year feature:\")\n",
    "    display_cols = ['DATE', 'Month-Year']\n",
    "    print(df[display_cols].head(10))\n",
    "    \n",
    "    # Show unique Month-Year values to verify\n",
    "    print(\"\\nUnique Month-Year values in the dataset:\")\n",
    "    print(sorted(df['Month-Year'].unique()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the feature engineering\n",
    "df_with_month_year = create_month_year_feature(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verification of Data Cleaning and Feature Engineering ===\n",
      "\n",
      "Dataset Shape: (329881, 9)\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Sample of Month-Year feature (first 5 rows):\n",
      "                 DATE   Month-Year\n",
      "0 2024-08-18 21:32:00  August 2024\n",
      "1 2024-08-18 21:32:00  August 2024\n",
      "2 2024-08-18 21:32:00  August 2024\n",
      "3 2024-08-18 21:32:00  August 2024\n",
      "4 2024-08-18 21:32:00  August 2024\n",
      "\n",
      "All Columns in Dataset:\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE', 'is_outlier', 'Month-Year']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Verification of Data Cleaning and Feature Engineering ===\")\n",
    "\n",
    "# 1. Basic Info\n",
    "print(\"\\nDataset Shape:\", df_with_month_year.shape)\n",
    "\n",
    "# 2. Check for missing values\n",
    "missing = df_with_month_year.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing[missing > 0] if missing.any() > 0 else \"No missing values\")\n",
    "\n",
    "# 3. Check for duplicates\n",
    "duplicates = df_with_month_year.duplicated().sum()\n",
    "print(\"\\nDuplicate Rows:\", duplicates)\n",
    "\n",
    "# 4. Show engineered Month-Year feature\n",
    "print(\"\\nSample of Month-Year feature (first 5 rows):\")\n",
    "print(df_with_month_year[['DATE', 'Month-Year']].head())\n",
    "\n",
    "# 5. Show all columns to confirm feature engineering\n",
    "print(\"\\nAll Columns in Dataset:\")\n",
    "print(df_with_month_year.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved successfully!\n",
      "\n",
      "File name: transaction_data_cleaned_and_engineered.csv\n",
      "Number of rows: 329881\n",
      "Number of columns: 9\n",
      "\n",
      "Columns saved:\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE', 'is_outlier', 'Month-Year']\n"
     ]
    }
   ],
   "source": [
    "# Save \n",
    "df_with_month_year.to_csv('transaction_data_cleaned_and_engineered.csv', index=False)\n",
    "\n",
    "print(\"Dataset has been saved successfully!\")\n",
    "print(f\"\\nFile name: transaction_data_cleaned_and_engineered.csv\")\n",
    "print(f\"Number of rows: {len(df_with_month_year)}\")\n",
    "print(f\"Number of columns: {len(df_with_month_year.columns)}\")\n",
    "print(\"\\nColumns saved:\")\n",
    "print(df_with_month_year.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
