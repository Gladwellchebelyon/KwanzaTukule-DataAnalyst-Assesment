{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Data Cleaning and Preparation\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Data cleaning and preparation is a crucial step in the data analysis process. It involves identifying and correcting errors, handling missing values, and transforming data into a suitable format for analysis.\n",
    "\n",
    "## Steps in Data Cleaning and Preparation\n",
    "\n",
    "### 1. Understanding the Data\n",
    "- **Data Collection**: Data has been downloaded through the provided link; Case Study Data - Read Only - case_study_data_2025-01-16T06_49_12.19881Z.csv.\n",
    "\n",
    "- **Data Description**: Understand the structure, types, and summary statistics of the data.\n",
    "\n",
    "### 2. Handling Missing Values\n",
    "- **Identify Missing Values**: Use methods like `.isnull()` or `.isna()` in pandas to find missing values.\n",
    "- **Imputation**: Fill missing values using techniques like mean, median, mode, or more sophisticated methods like KNN imputation.\n",
    "- **Removal**: Remove rows or columns with a high percentage of missing values.\n",
    "\n",
    "### 3. Removing Duplicates\n",
    "- **Identify Duplicates**: Use methods like `.duplicated()` in pandas.\n",
    "- **Remove Duplicates**: Use `.drop_duplicates()` to remove duplicate rows.\n",
    "\n",
    "### 4. Handling Outliers\n",
    "- **Identify Outliers**: Use statistical methods like Z-score or IQR to detect outliers.\n",
    "- **Treat Outliers**: Decide whether to remove, cap, or transform outliers.\n",
    "\n",
    "### 5. Data Transformation\n",
    "- **Normalization**: Scale data to a standard range, typically [0, 1].\n",
    "- **Standardization**: Transform data to have a mean of 0 and a standard deviation of 1.\n",
    "- **Encoding Categorical Variables**: Convert categorical data into numerical format using techniques like one-hot encoding or label encoding.\n",
    "\n",
    "### 6. Feature Engineering\n",
    "- **Creating New Features**: Derive new features from existing data to improve model performance.\n",
    "- **Feature Selection**: Select the most relevant features for analysis using methods like correlation analysis or feature importance from models.\n",
    "\n",
    "## Conclusion\n",
    "Effective data cleaning and preparation can significantly improve the quality of data and the performance of models. It is an iterative process that requires careful attention to detail and a good understanding of the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "file_path = \"Case Study Data - Read Only - case_study_data_2025-01-16T06_49_12.19881Z.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 333405 entries, 0 to 333404\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   DATE                 333405 non-null  object\n",
      " 1   ANONYMIZED CATEGORY  333405 non-null  object\n",
      " 2   ANONYMIZED PRODUCT   333405 non-null  object\n",
      " 3   ANONYMIZED BUSINESS  333405 non-null  object\n",
      " 4   ANONYMIZED LOCATION  333405 non-null  object\n",
      " 5   QUANTITY             333405 non-null  int64 \n",
      " 6   VALUE                333397 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 17.8+ MB\n",
      "None\n",
      "(333405, 7)\n",
      "                       DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "0  August 18, 2024, 9:32 PM        Category-106       Product-21f4   \n",
      "1  August 18, 2024, 9:32 PM        Category-120       Product-4156   \n",
      "2  August 18, 2024, 9:32 PM        Category-121       Product-49bd   \n",
      "3  August 18, 2024, 9:32 PM         Category-76       Product-61dd   \n",
      "4  August 18, 2024, 9:32 PM        Category-119       Product-66e0   \n",
      "\n",
      "  ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "0       Business-de42       Location-1ba8         1    850  \n",
      "1       Business-de42       Location-1ba8         2  1,910  \n",
      "2       Business-de42       Location-1ba8         1  3,670  \n",
      "3       Business-de42       Location-1ba8         1  2,605  \n",
      "4       Business-de42       Location-1ba8         5  1,480  \n",
      "            QUANTITY\n",
      "count  333405.000000\n",
      "mean        2.321186\n",
      "std         3.790614\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       359.000000\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(df.describe())\n",
    "\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Quality Report ===\n",
      "\n",
      "Dataset Shape: (333405, 7) (rows, columns)\n",
      "\n",
      "1. Missing Values:\n",
      "       Missing Count  Missing Percentage\n",
      "VALUE              8                 0.0\n",
      "\n",
      "2. Duplicate Rows:\n",
      "Total duplicates: 3524 (1.06%)\n",
      "\n",
      "3. Numerical Columns Summary:\n",
      "            QUANTITY\n",
      "count  333405.000000\n",
      "mean        2.321186\n",
      "std         3.790614\n",
      "min         0.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         2.000000\n",
      "max       359.000000\n",
      "\n",
      "4. Categorical Columns Summary:\n",
      "\n",
      "DATE:\n",
      "- Unique values: 96703\n",
      "\n",
      "ANONYMIZED CATEGORY:\n",
      "- Unique values: 46\n",
      "\n",
      "ANONYMIZED PRODUCT:\n",
      "- Unique values: 820\n",
      "\n",
      "ANONYMIZED BUSINESS:\n",
      "- Unique values: 4800\n",
      "\n",
      "ANONYMIZED LOCATION:\n",
      "- Unique values: 53\n",
      "\n",
      "VALUE:\n",
      "- Unique values: 1050\n",
      "3. Outliers (using IQR method):\n",
      "QUANTITY: 48631 outliers detected\n",
      "\n",
      "\n",
      "6. Class Distribution (for categorical columns):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def analyze_data_types(df):\n",
    "    print(\"\\n=== Column Data Types Analysis ===\")\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    print(\"\\nNumerical Columns:\")\n",
    "    for col in numeric_cols:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "    print(\"\\nCategorical Columns:\")\n",
    "    for col in categorical_cols:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "    return numeric_cols, categorical_cols\n",
    "\n",
    "def check_data_quality(df):\n",
    "    print(\"\\n=== Data Quality Report ===\")\n",
    "    \n",
    "    # Get total rows and columns\n",
    "    print(f\"\\nDataset Shape: {df.shape} (rows, columns)\")\n",
    "    \n",
    "    # 1. Missing Values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_percent = (missing / len(df)) * 100\n",
    "    missing_info = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing Percentage': missing_percent.round(2)\n",
    "    })\n",
    "    print(missing_info[missing_info['Missing Count'] > 0])\n",
    "    \n",
    "    # 2. Duplicates\n",
    "    print(\"\\n2. Duplicate Rows:\")\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"Total duplicates: {duplicates} ({(duplicates/len(df))*100:.2f}%)\")\n",
    "    \n",
    "    # 3. Basic Statistics for Numerical Columns\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\n3. Numerical Columns Summary:\")\n",
    "        print(df[numeric_cols].describe())\n",
    "    \n",
    "    # 4. Unique Values in Categorical Columns\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\n4. Categorical Columns Summary:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"- Unique values: {unique_count}\")\n",
    "            if unique_count < 10:  # Only show value counts if fewer than 10 unique values\n",
    "                print(\"- Value counts:\")\n",
    "                print(df[col].value_counts())\n",
    "    # 5. Outliers (for numerical columns)\n",
    "    print(\"3. Outliers (using IQR method):\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "        print(f\"{col}: {len(outliers)} outliers detected\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "     # 6. Class Imbalance (if target variable is specified)\n",
    "    print(\"6. Class Distribution (for categorical columns):\")\n",
    "    for col in categorical_cols:\n",
    "        balance = df[col].value_counts(normalize=True)\n",
    "        if len(balance) < 10:  # Only show if fewer than 10 unique values\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(balance.multiply(100).round(2).astype(str) + '%')\n",
    "            print(\"\\n\")\n",
    "    \n",
    "check_data_quality(df)\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# numeric_cols, categorical_cols = analyze_data_types(df)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Missing Values Analysis ===\n",
      "\n",
      "Before cleaning:\n",
      "Missing values in each column:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_missing_values(df):\n",
    "    print(\"\\n=== Missing Values Analysis ===\")\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    df_temp = df.copy()\n",
    "    \n",
    "    # Convert VALUE to numeric, handling the commas\n",
    "    df_temp['VALUE'] = df_temp['VALUE'].replace({',': ''}, regex=True)\n",
    "    \n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(\"Missing values in each column:\")\n",
    "    print(df_temp.isnull().sum())\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "# Run it\n",
    "df_temp = check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Duplicates Analysis ===\n",
      "\n",
      "Total number of duplicate rows: 3524\n",
      "\n",
      "First few duplicate rows:\n",
      "                            DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "6153   January 6, 2024, 11:52 AM         Category-91       Product-1b48   \n",
      "7554       July 9, 2024, 2:26 PM        Category-104       Product-af50   \n",
      "7555       July 9, 2024, 2:26 PM         Category-92       Product-d09a   \n",
      "12238    April 19, 2024, 3:19 PM         Category-75       Product-086d   \n",
      "12239    April 19, 2024, 3:19 PM        Category-106       Product-21f4   \n",
      "\n",
      "      ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "6153        Business-20fc       Location-b125         1  3,680  \n",
      "7554        Business-476c       Location-b27b         1  1,310  \n",
      "7555        Business-476c       Location-b27b         1  1,550  \n",
      "12238       Business-b48e       Location-03fc         3  2,090  \n",
      "12239       Business-b48e       Location-03fc         2    850  \n",
      "\n",
      "Duplicate counts by column combinations:\n",
      "DATE: 236702 duplicates\n",
      "ANONYMIZED CATEGORY: 333359 duplicates\n",
      "ANONYMIZED PRODUCT: 332585 duplicates\n",
      "ANONYMIZED BUSINESS: 328605 duplicates\n",
      "ANONYMIZED LOCATION: 333352 duplicates\n",
      "QUANTITY: 333326 duplicates\n",
      "VALUE: 332354 duplicates\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(df):\n",
    "    print(\"\\n=== Duplicates Analysis ===\")\n",
    "    \n",
    "    # Count total duplicates\n",
    "    dup_count = df.duplicated().sum()\n",
    "    print(f\"\\nTotal number of duplicate rows: {dup_count}\")\n",
    "    \n",
    "    # Show example of duplicates\n",
    "    if dup_count > 0:\n",
    "        print(\"\\nFirst few duplicate rows:\")\n",
    "        duplicates = df[df.duplicated(keep='first')]\n",
    "        print(duplicates.head())\n",
    "        \n",
    "        # Check which columns are most commonly duplicated\n",
    "        print(\"\\nDuplicate counts by column combinations:\")\n",
    "        for col in df.columns:\n",
    "            dup_by_col = df.duplicated(subset=[col]).sum()\n",
    "            print(f\"{col}: {dup_by_col} duplicates\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "# Run it\n",
    "duplicates = check_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUANTITY Outliers Analysis ===\n",
      "\n",
      "QUANTITY statistics:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "Outlier boundaries:\n",
      "Lower bound: -0.5\n",
      "Upper bound: 3.5\n",
      "\n",
      "Number of outliers: 48631\n",
      "\n",
      "Outlier QUANTITY values:\n",
      "QUANTITY\n",
      "5     18511\n",
      "4      9834\n",
      "10     8232\n",
      "6      3262\n",
      "7      1648\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def check_outliers(df):\n",
    "    print(\"\\n=== QUANTITY Outliers Analysis ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nQUANTITY statistics:\")\n",
    "    print(df['QUANTITY'].describe())\n",
    "    \n",
    "    # Calculate outlier boundaries\n",
    "    Q1 = df['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    print(f\"\\nOutlier boundaries:\")\n",
    "    print(f\"Lower bound: {lower_bound}\")\n",
    "    print(f\"Upper bound: {upper_bound}\")\n",
    "    \n",
    "    # Find outliers\n",
    "    outliers = df[(df['QUANTITY'] < lower_bound) | (df['QUANTITY'] > upper_bound)]\n",
    "    print(f\"\\nNumber of outliers: {len(outliers)}\")\n",
    "    \n",
    "    # Show distribution of outlier quantities\n",
    "    print(\"\\nOutlier QUANTITY values:\")\n",
    "    print(outliers['QUANTITY'].value_counts().head())\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "# Run it\n",
    "outliers = check_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning process...\n",
      "\n",
      "=== Missing Values Analysis ===\n",
      "\n",
      "Before cleaning:\n",
      "Missing values in each column:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n",
      "\n",
      "=== Duplicates Analysis ===\n",
      "\n",
      "Total number of duplicate rows: 3524\n",
      "\n",
      "First few duplicate rows:\n",
      "                            DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
      "6153   January 6, 2024, 11:52 AM         Category-91       Product-1b48   \n",
      "7554       July 9, 2024, 2:26 PM        Category-104       Product-af50   \n",
      "7555       July 9, 2024, 2:26 PM         Category-92       Product-d09a   \n",
      "12238    April 19, 2024, 3:19 PM         Category-75       Product-086d   \n",
      "12239    April 19, 2024, 3:19 PM        Category-106       Product-21f4   \n",
      "\n",
      "      ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY  VALUE  \n",
      "6153        Business-20fc       Location-b125         1  3,680  \n",
      "7554        Business-476c       Location-b27b         1  1,310  \n",
      "7555        Business-476c       Location-b27b         1  1,550  \n",
      "12238       Business-b48e       Location-03fc         3  2,090  \n",
      "12239       Business-b48e       Location-03fc         2    850  \n",
      "\n",
      "Duplicate counts by column combinations:\n",
      "DATE: 236702 duplicates\n",
      "ANONYMIZED CATEGORY: 333359 duplicates\n",
      "ANONYMIZED PRODUCT: 332585 duplicates\n",
      "ANONYMIZED BUSINESS: 328605 duplicates\n",
      "ANONYMIZED LOCATION: 333352 duplicates\n",
      "QUANTITY: 333326 duplicates\n",
      "VALUE: 332354 duplicates\n",
      "\n",
      "=== QUANTITY Outliers Analysis ===\n",
      "\n",
      "QUANTITY statistics:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "Outlier boundaries:\n",
      "Lower bound: -0.5\n",
      "Upper bound: 3.5\n",
      "\n",
      "Number of outliers: 48631\n",
      "\n",
      "Outlier QUANTITY values:\n",
      "QUANTITY\n",
      "5     18511\n",
      "4      9834\n",
      "10     8232\n",
      "6      3262\n",
      "7      1648\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Cleaning Data Step by Step ===\n",
      "\n",
      "Step 1: Cleaning VALUE column\n",
      "VALUE column cleaned\n",
      "\n",
      "Step 2: Removing duplicates\n",
      "Removed 3524 duplicate rows\n",
      "\n",
      "Step 3: Flagging outliers\n",
      "Flagged 48115 outliers\n",
      "\n",
      "Cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "def clean_data_step_by_step(df):\n",
    "    print(\"\\n=== Cleaning Data Step by Step ===\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Clean VALUE column\n",
    "    print(\"\\nStep 1: Cleaning VALUE column\")\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].replace({',': ''}, regex=True)\n",
    "    print(\"VALUE column cleaned\")\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    print(\"\\nStep 2: Removing duplicates\")\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    rows_removed = initial_rows - len(df_clean)\n",
    "    print(f\"Removed {rows_removed} duplicate rows\")\n",
    "    \n",
    "    # 3. Flag outliers\n",
    "    print(\"\\nStep 3: Flagging outliers\")\n",
    "    Q1 = df_clean['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df_clean['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    df_clean['is_outlier'] = ((df_clean['QUANTITY'] < lower_bound) | \n",
    "                             (df_clean['QUANTITY'] > upper_bound))\n",
    "    \n",
    "    outlier_count = df_clean['is_outlier'].sum()\n",
    "    print(f\"Flagged {outlier_count} outliers\")\n",
    "    \n",
    "    print(\"\\nCleaning complete!\")\n",
    "    return df_clean\n",
    "\n",
    "# Run the complete analysis\n",
    "print(\"Starting data cleaning process...\")\n",
    "df_temp = check_missing_values(df)\n",
    "duplicates = check_duplicates(df)\n",
    "outliers = check_outliers(df)\n",
    "df_clean = clean_data_step_by_step(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cleaning Verification Report ===\n",
      "\n",
      "1. Row Count Comparison:\n",
      "Original rows: 333405\n",
      "Cleaned rows: 329881\n",
      "Rows removed: 3524\n",
      "\n",
      "2. Missing Values Check:\n",
      "\n",
      "Before cleaning:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "dtype: int64\n",
      "\n",
      "After cleaning:\n",
      "DATE                   0\n",
      "ANONYMIZED CATEGORY    0\n",
      "ANONYMIZED PRODUCT     0\n",
      "ANONYMIZED BUSINESS    0\n",
      "ANONYMIZED LOCATION    0\n",
      "QUANTITY               0\n",
      "VALUE                  8\n",
      "is_outlier             0\n",
      "dtype: int64\n",
      "\n",
      "3. Duplicates Check:\n",
      "Original duplicates: 3524\n",
      "Remaining duplicates: 0\n",
      "\n",
      "4. QUANTITY Statistics Comparison:\n",
      "\n",
      "Before cleaning:\n",
      "count    333405.000000\n",
      "mean          2.321186\n",
      "std           3.790614\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "After cleaning:\n",
      "count    329881.000000\n",
      "mean          2.321507\n",
      "std           3.767796\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "5. Outlier Flags:\n",
      "Number of rows flagged as outliers: 48115\n",
      "Percentage of outliers: 14.59%\n",
      "\n",
      "6. VALUE Column Format:\n",
      "\n",
      "Before cleaning (first 5 unique values):\n",
      "['850' '1,910' '3,670' '2,605' '1,480']\n",
      "\n",
      "After cleaning (first 5 unique values):\n",
      "['850' '1910' '3670' '2605' '1480']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verify_cleaning(df_original, df_cleaned):\n",
    "    print(\"\\n=== Cleaning Verification Report ===\")\n",
    "    \n",
    "    # 1. Compare row counts\n",
    "    print(\"\\n1. Row Count Comparison:\")\n",
    "    print(f\"Original rows: {len(df_original)}\")\n",
    "    print(f\"Cleaned rows: {len(df_cleaned)}\")\n",
    "    print(f\"Rows removed: {len(df_original) - len(df_cleaned)}\")\n",
    "    \n",
    "    # 2. Check if missing values were handled\n",
    "    print(\"\\n2. Missing Values Check:\")\n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(df_original.isnull().sum())\n",
    "    print(\"\\nAfter cleaning:\")\n",
    "    print(df_cleaned.isnull().sum())\n",
    "    \n",
    "    # 3. Check for duplicates\n",
    "    print(\"\\n3. Duplicates Check:\")\n",
    "    print(f\"Original duplicates: {df_original.duplicated().sum()}\")\n",
    "    print(f\"Remaining duplicates: {df_cleaned.duplicated().sum()}\")\n",
    "    \n",
    "    # 4. Compare QUANTITY statistics\n",
    "    print(\"\\n4. QUANTITY Statistics Comparison:\")\n",
    "    print(\"\\nBefore cleaning:\")\n",
    "    print(df_original['QUANTITY'].describe())\n",
    "    print(\"\\nAfter cleaning:\")\n",
    "    print(df_cleaned['QUANTITY'].describe())\n",
    "    \n",
    "    # 5. Check outlier flags\n",
    "    if 'is_outlier' in df_cleaned.columns:\n",
    "        print(\"\\n5. Outlier Flags:\")\n",
    "        outlier_count = df_cleaned['is_outlier'].sum()\n",
    "        print(f\"Number of rows flagged as outliers: {outlier_count}\")\n",
    "        print(f\"Percentage of outliers: {(outlier_count/len(df_cleaned))*100:.2f}%\")\n",
    "    \n",
    "    # 6. Check VALUE column format\n",
    "    print(\"\\n6. VALUE Column Format:\")\n",
    "    print(\"\\nBefore cleaning (first 5 unique values):\")\n",
    "    print(df_original['VALUE'].unique()[:5])\n",
    "    print(\"\\nAfter cleaning (first 5 unique values):\")\n",
    "    print(df_cleaned['VALUE'].unique()[:5])\n",
    "\n",
    "# Run the verification\n",
    "verify_cleaning(df, df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Cleaning Steps ===\n",
      "Converting VALUE to numeric...\n",
      "Handling missing values...\n",
      "Removing duplicates...\n",
      "\n",
      "Cleaning complete! Let's verify the results...\n",
      "\n",
      "Verification:\n",
      "Missing values remaining: 0\n",
      "VALUE dtype: float64\n",
      "Sample of VALUE column (first 5):\n",
      "0     850.0\n",
      "1    1910.0\n",
      "2    3670.0\n",
      "3    2605.0\n",
      "4    1480.0\n",
      "Name: VALUE, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def final_cleaning(df):\n",
    "    print(\"\\n=== Final Cleaning Steps ===\")\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # 1. Properly convert VALUE to numeric\n",
    "    print(\"Converting VALUE to numeric...\")\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].str.replace(',', '').astype(float)\n",
    "    \n",
    "    # 2. Handle missing values in VALUE\n",
    "    print(\"Handling missing values...\")\n",
    "    value_median = df_clean['VALUE'].median()\n",
    "    df_clean['VALUE'] = df_clean['VALUE'].fillna(value_median)\n",
    "    \n",
    "    # 3. Remove duplicates (already done but included for completeness)\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    # 4. Flag outliers (already done but included for completeness)\n",
    "    Q1 = df_clean['QUANTITY'].quantile(0.25)\n",
    "    Q3 = df_clean['QUANTITY'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    df_clean['is_outlier'] = ((df_clean['QUANTITY'] < lower_bound) | \n",
    "                             (df_clean['QUANTITY'] > upper_bound))\n",
    "    \n",
    "    print(\"\\nCleaning complete! Let's verify the results...\")\n",
    "    \n",
    "    # Quick verification\n",
    "    print(\"\\nVerification:\")\n",
    "    print(f\"Missing values remaining: {df_clean.isnull().sum().sum()}\")\n",
    "    print(f\"VALUE dtype: {df_clean['VALUE'].dtype}\")\n",
    "    print(f\"Sample of VALUE column (first 5):\")\n",
    "    print(df_clean['VALUE'].head())\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Run the final cleaning\n",
    "df_final = final_cleaning(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Clean Data Summary ===\n",
      "\n",
      "1. Dataset Shape:\n",
      "Rows: 329881\n",
      "Columns: 8\n",
      "\n",
      "2. Data Types:\n",
      "DATE                    object\n",
      "ANONYMIZED CATEGORY     object\n",
      "ANONYMIZED PRODUCT      object\n",
      "ANONYMIZED BUSINESS     object\n",
      "ANONYMIZED LOCATION     object\n",
      "QUANTITY                 int64\n",
      "VALUE                  float64\n",
      "is_outlier                bool\n",
      "dtype: object\n",
      "\n",
      "3. Value Ranges:\n",
      "\n",
      "QUANTITY:\n",
      "count    329881.000000\n",
      "mean          2.321507\n",
      "std           3.767796\n",
      "min           0.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           2.000000\n",
      "max         359.000000\n",
      "Name: QUANTITY, dtype: float64\n",
      "\n",
      "VALUE:\n",
      "count    329881.000000\n",
      "mean       2319.004962\n",
      "std        1582.561268\n",
      "min           0.000000\n",
      "25%        1420.000000\n",
      "50%        1840.000000\n",
      "75%        2750.000000\n",
      "max       16136.000000\n",
      "Name: VALUE, dtype: float64\n",
      "\n",
      "4. Outlier Distribution:\n",
      "Normal transactions: 281766\n",
      "Outlier transactions: 48115\n"
     ]
    }
   ],
   "source": [
    "def summarize_clean_data(df):\n",
    "    print(\"\\n=== Clean Data Summary ===\")\n",
    "    \n",
    "    print(\"\\n1. Dataset Shape:\")\n",
    "    print(f\"Rows: {len(df)}\")\n",
    "    print(f\"Columns: {len(df.columns)}\")\n",
    "    \n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\n3. Value Ranges:\")\n",
    "    numeric_cols = ['QUANTITY', 'VALUE']\n",
    "    for col in numeric_cols:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(df[col].describe())\n",
    "    \n",
    "    print(\"\\n4. Outlier Distribution:\")\n",
    "    print(\"Normal transactions:\", len(df[~df['is_outlier']]))\n",
    "    print(\"Outlier transactions:\", len(df[df['is_outlier']]))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run summary\n",
    "df_summary = summarize_clean_data(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FEATURE ENGINEERING\n",
    "\n",
    "- Creating New Features\n",
    "\n",
    "One common feature engineering task is to extract the month and year from a date column. This can be useful for time series analysis or aggregating data by month and year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering process...\n",
      "\n",
      "=== Creating Time-Based Features ===\n",
      "Created time features: year, month, day, day_of_week, is_weekend, quarter\n",
      "\n",
      "=== Creating Transaction Features ===\n",
      "Created features: price_per_unit, transaction_size, order_size\n",
      "\n",
      "=== Creating Business Features ===\n",
      "Created features: business_frequency, business_avg_value\n",
      "\n",
      "=== Creating Product Features ===\n",
      "Created features: product_popularity, product_avg_price\n",
      "\n",
      "=== Creating Location Features ===\n",
      "Created features: location_frequency, location_avg_value\n",
      "\n",
      "=== New Features Summary ===\n",
      "\n",
      "New columns added:\n",
      "['business_avg_value', 'business_frequency', 'day', 'day_of_week', 'is_weekend', 'location_avg_value', 'location_frequency', 'month', 'order_size', 'price_per_unit', 'product_avg_price', 'product_popularity', 'quarter', 'transaction_size', 'year']\n",
      "\n",
      "Dataset shape:\n",
      "Original: (329881, 8)\n",
      "After feature engineering: (329881, 23)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def create_time_features(df):\n",
    "    print(\"\\n=== Creating Time-Based Features ===\")\n",
    "    \n",
    "    # Convert DATE to datetime if not already\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # Extract time components\n",
    "    df['year'] = df['DATE'].dt.year\n",
    "    df['month'] = df['DATE'].dt.month\n",
    "    df['day'] = df['DATE'].dt.day\n",
    "    df['day_of_week'] = df['DATE'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['quarter'] = df['DATE'].dt.quarter\n",
    "    \n",
    "    print(\"Created time features: year, month, day, day_of_week, is_weekend, quarter\")\n",
    "    return df\n",
    "\n",
    "def create_transaction_features(df):\n",
    "    print(\"\\n=== Creating Transaction Features ===\")\n",
    "    \n",
    "    # Price per unit\n",
    "    df['price_per_unit'] = df['VALUE'] / df['QUANTITY']\n",
    "    \n",
    "    # Transaction size categories\n",
    "    df['transaction_size'] = pd.qcut(df['VALUE'], q=5, labels=['Very Small', 'Small', 'Medium', 'Large', 'Very Large'])\n",
    "    \n",
    "    # Order size categories\n",
    "    df['order_size'] = pd.qcut(df['QUANTITY'], q=3, labels=['Small', 'Medium', 'Large'])\n",
    "    \n",
    "    print(\"Created features: price_per_unit, transaction_size, order_size\")\n",
    "    return df\n",
    "\n",
    "def create_business_features(df):\n",
    "    print(\"\\n=== Creating Business Features ===\")\n",
    "    \n",
    "    # Business frequency\n",
    "    business_freq = df.groupby('ANONYMIZED BUSINESS').size()\n",
    "    df['business_frequency'] = df['ANONYMIZED BUSINESS'].map(business_freq)\n",
    "    \n",
    "    # Business average transaction value\n",
    "    business_avg_value = df.groupby('ANONYMIZED BUSINESS')['VALUE'].mean()\n",
    "    df['business_avg_value'] = df['ANONYMIZED BUSINESS'].map(business_avg_value)\n",
    "    \n",
    "    print(\"Created features: business_frequency, business_avg_value\")\n",
    "    return df\n",
    "\n",
    "def create_product_features(df):\n",
    "    print(\"\\n=== Creating Product Features ===\")\n",
    "    \n",
    "    # Product popularity\n",
    "    product_freq = df.groupby('ANONYMIZED PRODUCT').size()\n",
    "    df['product_popularity'] = df['ANONYMIZED PRODUCT'].map(product_freq)\n",
    "    \n",
    "    # Product average price\n",
    "    product_avg_price = df.groupby('ANONYMIZED PRODUCT')['price_per_unit'].mean()\n",
    "    df['product_avg_price'] = df['ANONYMIZED PRODUCT'].map(product_avg_price)\n",
    "    \n",
    "    print(\"Created features: product_popularity, product_avg_price\")\n",
    "    return df\n",
    "\n",
    "def create_location_features(df):\n",
    "    print(\"\\n=== Creating Location Features ===\")\n",
    "    \n",
    "    # Location transaction frequency\n",
    "    location_freq = df.groupby('ANONYMIZED LOCATION').size()\n",
    "    df['location_frequency'] = df['ANONYMIZED LOCATION'].map(location_freq)\n",
    "    \n",
    "    # Location average transaction value\n",
    "    location_avg_value = df.groupby('ANONYMIZED LOCATION')['VALUE'].mean()\n",
    "    df['location_avg_value'] = df['ANONYMIZED LOCATION'].map(location_avg_value)\n",
    "    \n",
    "    print(\"Created features: location_frequency, location_avg_value\")\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    print(\"Starting feature engineering process...\")\n",
    "    df_featured = df.copy()\n",
    "    \n",
    "    # Apply all feature engineering steps\n",
    "    df_featured = create_time_features(df_featured)\n",
    "    df_featured = create_transaction_features(df_featured)\n",
    "    df_featured = create_business_features(df_featured)\n",
    "    df_featured = create_product_features(df_featured)\n",
    "    df_featured = create_location_features(df_featured)\n",
    "    \n",
    "    # Show new features summary\n",
    "    print(\"\\n=== New Features Summary ===\")\n",
    "    print(\"\\nNew columns added:\")\n",
    "    new_columns = set(df_featured.columns) - set(df.columns)\n",
    "    print(sorted(list(new_columns)))\n",
    "    \n",
    "    print(\"\\nDataset shape:\")\n",
    "    print(f\"Original: {df.shape}\")\n",
    "    print(f\"After feature engineering: {df_featured.shape}\")\n",
    "    \n",
    "    return df_featured\n",
    "\n",
    "# Run feature engineering\n",
    "df_with_features = feature_engineering(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== New Features Analysis ===\n",
      "\n",
      "Transactions by day of week:\n",
      "             count         mean\n",
      "day_of_week                    \n",
      "0            58037  2275.572859\n",
      "1            52028  2362.002076\n",
      "2            50666  2319.359097\n",
      "3            54499  2348.988018\n",
      "4            55990  2318.728076\n",
      "5             2315  2242.573218\n",
      "6            56346  2298.135129\n",
      "\n",
      "Weekday vs Weekend transactions:\n",
      "             count         mean\n",
      "is_weekend                     \n",
      "0           271220  2323.993057\n",
      "1            58661  2295.942432\n",
      "\n",
      "Transaction size distribution:\n",
      "transaction_size\n",
      "Large         0.204177\n",
      "Small         0.203734\n",
      "Very Small    0.200027\n",
      "Medium        0.196556\n",
      "Very Large    0.195507\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Price per unit statistics:\n",
      "count    329881.0\n",
      "mean          inf\n",
      "std           NaN\n",
      "min           0.0\n",
      "25%         657.5\n",
      "50%        1380.0\n",
      "75%        2130.0\n",
      "max           inf\n",
      "Name: price_per_unit, dtype: float64\n",
      "\n",
      "Top 5 locations by frequency:\n",
      "88979     43891\n",
      "67792     43891\n",
      "255860    43891\n",
      "255859    43891\n",
      "310437    43891\n",
      "Name: location_frequency, dtype: int64\n",
      "\n",
      "Top 5 most popular products:\n",
      "195386    24736\n",
      "115409    24736\n",
      "144761    24736\n",
      "295618    24736\n",
      "215639    24736\n",
      "Name: product_popularity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def analyze_new_features(df):\n",
    "    print(\"\\n=== New Features Analysis ===\")\n",
    "    \n",
    "    # 1. Time-based patterns\n",
    "    print(\"\\nTransactions by day of week:\")\n",
    "    print(df.groupby('day_of_week')['VALUE'].agg(['count', 'mean']))\n",
    "    \n",
    "    print(\"\\nWeekday vs Weekend transactions:\")\n",
    "    print(df.groupby('is_weekend')['VALUE'].agg(['count', 'mean']))\n",
    "    \n",
    "    # 2. Transaction size distribution\n",
    "    print(\"\\nTransaction size distribution:\")\n",
    "    print(df['transaction_size'].value_counts(normalize=True))\n",
    "    \n",
    "    # 3. Price per unit statistics\n",
    "    print(\"\\nPrice per unit statistics:\")\n",
    "    print(df['price_per_unit'].describe())\n",
    "    \n",
    "    # 4. Location analysis\n",
    "    print(\"\\nTop 5 locations by frequency:\")\n",
    "    print(df['location_frequency'].sort_values(ascending=False).head())\n",
    "    \n",
    "    # 5. Product popularity\n",
    "    print(\"\\nTop 5 most popular products:\")\n",
    "    print(df['product_popularity'].sort_values(ascending=False).head())\n",
    "\n",
    "# Run analysis\n",
    "analyze_new_features(df_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dates from my dataset:\n",
      "0    August 18, 2024, 9:32 PM\n",
      "1    August 18, 2024, 9:32 PM\n",
      "2    August 18, 2024, 9:32 PM\n",
      "3    August 18, 2024, 9:32 PM\n",
      "4    August 18, 2024, 9:32 PM\n",
      "Name: DATE, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the first few dates to see their format\n",
    "print(\"Sample dates from my dataset:\")\n",
    "print(df_final['DATE'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Month-Year Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Month-Year Feature ===\n",
      "\n",
      "Sample of the new Month-Year feature:\n",
      "                 DATE   Month-Year\n",
      "0 2024-08-18 21:32:00  August 2024\n",
      "1 2024-08-18 21:32:00  August 2024\n",
      "2 2024-08-18 21:32:00  August 2024\n",
      "3 2024-08-18 21:32:00  August 2024\n",
      "4 2024-08-18 21:32:00  August 2024\n",
      "5 2024-08-18 21:32:00  August 2024\n",
      "6 2024-08-18 21:32:00  August 2024\n",
      "7 2024-08-18 21:32:00  August 2024\n",
      "8 2024-08-18 21:32:00  August 2024\n",
      "9 2024-08-18 21:32:00  August 2024\n",
      "\n",
      "Unique Month-Year values in the dataset:\n",
      "['April 2024', 'August 2024', 'December 2024', 'February 2024', 'January 2024', 'July 2024', 'June 2024', 'March 2024', 'May 2024', 'November 2024', 'October 2024', 'September 2024']\n"
     ]
    }
   ],
   "source": [
    "def create_month_year_feature(df):\n",
    "    print(\"\\n=== Creating Month-Year Feature ===\")\n",
    "    \n",
    "    # Convert 'DATE' column to datetime if it's not already\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    # Create 'Month-Year' column\n",
    "    df['Month-Year'] = df['DATE'].dt.strftime('%B %Y')\n",
    "    \n",
    "    # Display sample results\n",
    "    print(\"\\nSample of the new Month-Year feature:\")\n",
    "    display_cols = ['DATE', 'Month-Year']\n",
    "    print(df[display_cols].head(10))\n",
    "    \n",
    "    # Show unique Month-Year values to verify\n",
    "    print(\"\\nUnique Month-Year values in the dataset:\")\n",
    "    print(sorted(df['Month-Year'].unique()))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run the feature engineering\n",
    "df_with_month_year = create_month_year_feature(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>ANONYMIZED CATEGORY</th>\n",
       "      <th>ANONYMIZED PRODUCT</th>\n",
       "      <th>ANONYMIZED BUSINESS</th>\n",
       "      <th>ANONYMIZED LOCATION</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>VALUE</th>\n",
       "      <th>is_outlier</th>\n",
       "      <th>Month-Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-106</td>\n",
       "      <td>Product-21f4</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>850.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-120</td>\n",
       "      <td>Product-4156</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>2</td>\n",
       "      <td>1910.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-121</td>\n",
       "      <td>Product-49bd</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>3670.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-76</td>\n",
       "      <td>Product-61dd</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>2605.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-119</td>\n",
       "      <td>Product-66e0</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>5</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>True</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-76</td>\n",
       "      <td>Product-6e9c</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>2605.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-120</td>\n",
       "      <td>Product-7864</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-119</td>\n",
       "      <td>Product-7940</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>4</td>\n",
       "      <td>1460.0</td>\n",
       "      <td>True</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-96</td>\n",
       "      <td>Product-87b2</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>805.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-08-18 21:32:00</td>\n",
       "      <td>Category-106</td>\n",
       "      <td>Product-c14c</td>\n",
       "      <td>Business-de42</td>\n",
       "      <td>Location-1ba8</td>\n",
       "      <td>1</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-08-06 19:36:00</td>\n",
       "      <td>Category-100</td>\n",
       "      <td>Product-3cc2</td>\n",
       "      <td>Business-7488</td>\n",
       "      <td>Location-4ea1</td>\n",
       "      <td>2</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2024-08-06 19:36:00</td>\n",
       "      <td>Category-85</td>\n",
       "      <td>Product-5ab4</td>\n",
       "      <td>Business-7488</td>\n",
       "      <td>Location-4ea1</td>\n",
       "      <td>1</td>\n",
       "      <td>3650.0</td>\n",
       "      <td>False</td>\n",
       "      <td>August 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-06-23 19:37:00</td>\n",
       "      <td>Category-120</td>\n",
       "      <td>Product-14f3</td>\n",
       "      <td>Business-2460</td>\n",
       "      <td>Location-4ea1</td>\n",
       "      <td>1</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>False</td>\n",
       "      <td>June 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-06-23 19:37:00</td>\n",
       "      <td>Category-75</td>\n",
       "      <td>Product-8f75</td>\n",
       "      <td>Business-2460</td>\n",
       "      <td>Location-4ea1</td>\n",
       "      <td>1</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>June 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-11-25 16:42:00</td>\n",
       "      <td>Category-82</td>\n",
       "      <td>Product-91f3</td>\n",
       "      <td>Business-aab8</td>\n",
       "      <td>Location-128a</td>\n",
       "      <td>1</td>\n",
       "      <td>815.0</td>\n",
       "      <td>False</td>\n",
       "      <td>November 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2024-11-25 16:42:00</td>\n",
       "      <td>Category-77</td>\n",
       "      <td>Product-b1c8</td>\n",
       "      <td>Business-aab8</td>\n",
       "      <td>Location-128a</td>\n",
       "      <td>1</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>False</td>\n",
       "      <td>November 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2024-11-25 16:42:00</td>\n",
       "      <td>Category-78</td>\n",
       "      <td>Product-b409</td>\n",
       "      <td>Business-aab8</td>\n",
       "      <td>Location-128a</td>\n",
       "      <td>1</td>\n",
       "      <td>750.0</td>\n",
       "      <td>False</td>\n",
       "      <td>November 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-09-06 08:42:00</td>\n",
       "      <td>Category-75</td>\n",
       "      <td>Product-6aa1</td>\n",
       "      <td>Business-f13b</td>\n",
       "      <td>Location-bb69</td>\n",
       "      <td>1</td>\n",
       "      <td>2255.0</td>\n",
       "      <td>False</td>\n",
       "      <td>September 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-09-06 08:42:00</td>\n",
       "      <td>Category-76</td>\n",
       "      <td>Product-c570</td>\n",
       "      <td>Business-f13b</td>\n",
       "      <td>Location-bb69</td>\n",
       "      <td>1</td>\n",
       "      <td>2540.0</td>\n",
       "      <td>False</td>\n",
       "      <td>September 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2024-09-06 08:42:00</td>\n",
       "      <td>Category-100</td>\n",
       "      <td>Product-f3ee</td>\n",
       "      <td>Business-f13b</td>\n",
       "      <td>Location-bb69</td>\n",
       "      <td>1</td>\n",
       "      <td>1880.0</td>\n",
       "      <td>False</td>\n",
       "      <td>September 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2024-10-27 21:05:00</td>\n",
       "      <td>Category-95</td>\n",
       "      <td>Product-766b</td>\n",
       "      <td>Business-5de0</td>\n",
       "      <td>Location-4ea1</td>\n",
       "      <td>1</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>False</td>\n",
       "      <td>October 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2024-09-05 19:48:00</td>\n",
       "      <td>Category-120</td>\n",
       "      <td>Product-14f3</td>\n",
       "      <td>Business-5d3e</td>\n",
       "      <td>Location-1979</td>\n",
       "      <td>5</td>\n",
       "      <td>1740.0</td>\n",
       "      <td>True</td>\n",
       "      <td>September 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2024-09-05 19:48:00</td>\n",
       "      <td>Category-120</td>\n",
       "      <td>Product-9a3e</td>\n",
       "      <td>Business-5d3e</td>\n",
       "      <td>Location-1979</td>\n",
       "      <td>5</td>\n",
       "      <td>1695.0</td>\n",
       "      <td>True</td>\n",
       "      <td>September 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-05-23 20:22:00</td>\n",
       "      <td>Category-75</td>\n",
       "      <td>Product-086d</td>\n",
       "      <td>Business-22a2</td>\n",
       "      <td>Location-c2f2</td>\n",
       "      <td>5</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>True</td>\n",
       "      <td>May 2024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2024-05-23 20:22:00</td>\n",
       "      <td>Category-91</td>\n",
       "      <td>Product-32b3</td>\n",
       "      <td>Business-22a2</td>\n",
       "      <td>Location-c2f2</td>\n",
       "      <td>3</td>\n",
       "      <td>640.0</td>\n",
       "      <td>False</td>\n",
       "      <td>May 2024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  DATE ANONYMIZED CATEGORY ANONYMIZED PRODUCT  \\\n",
       "0  2024-08-18 21:32:00        Category-106       Product-21f4   \n",
       "1  2024-08-18 21:32:00        Category-120       Product-4156   \n",
       "2  2024-08-18 21:32:00        Category-121       Product-49bd   \n",
       "3  2024-08-18 21:32:00         Category-76       Product-61dd   \n",
       "4  2024-08-18 21:32:00        Category-119       Product-66e0   \n",
       "5  2024-08-18 21:32:00         Category-76       Product-6e9c   \n",
       "6  2024-08-18 21:32:00        Category-120       Product-7864   \n",
       "7  2024-08-18 21:32:00        Category-119       Product-7940   \n",
       "8  2024-08-18 21:32:00         Category-96       Product-87b2   \n",
       "9  2024-08-18 21:32:00        Category-106       Product-c14c   \n",
       "10 2024-08-06 19:36:00        Category-100       Product-3cc2   \n",
       "11 2024-08-06 19:36:00         Category-85       Product-5ab4   \n",
       "12 2024-06-23 19:37:00        Category-120       Product-14f3   \n",
       "13 2024-06-23 19:37:00         Category-75       Product-8f75   \n",
       "14 2024-11-25 16:42:00         Category-82       Product-91f3   \n",
       "15 2024-11-25 16:42:00         Category-77       Product-b1c8   \n",
       "16 2024-11-25 16:42:00         Category-78       Product-b409   \n",
       "17 2024-09-06 08:42:00         Category-75       Product-6aa1   \n",
       "18 2024-09-06 08:42:00         Category-76       Product-c570   \n",
       "19 2024-09-06 08:42:00        Category-100       Product-f3ee   \n",
       "20 2024-10-27 21:05:00         Category-95       Product-766b   \n",
       "21 2024-09-05 19:48:00        Category-120       Product-14f3   \n",
       "22 2024-09-05 19:48:00        Category-120       Product-9a3e   \n",
       "23 2024-05-23 20:22:00         Category-75       Product-086d   \n",
       "24 2024-05-23 20:22:00         Category-91       Product-32b3   \n",
       "\n",
       "   ANONYMIZED BUSINESS ANONYMIZED LOCATION  QUANTITY   VALUE  is_outlier  \\\n",
       "0        Business-de42       Location-1ba8         1   850.0       False   \n",
       "1        Business-de42       Location-1ba8         2  1910.0       False   \n",
       "2        Business-de42       Location-1ba8         1  3670.0       False   \n",
       "3        Business-de42       Location-1ba8         1  2605.0       False   \n",
       "4        Business-de42       Location-1ba8         5  1480.0        True   \n",
       "5        Business-de42       Location-1ba8         1  2605.0       False   \n",
       "6        Business-de42       Location-1ba8         1  1940.0       False   \n",
       "7        Business-de42       Location-1ba8         4  1460.0        True   \n",
       "8        Business-de42       Location-1ba8         1   805.0       False   \n",
       "9        Business-de42       Location-1ba8         1  1350.0       False   \n",
       "10       Business-7488       Location-4ea1         2  1700.0       False   \n",
       "11       Business-7488       Location-4ea1         1  3650.0       False   \n",
       "12       Business-2460       Location-4ea1         1  1800.0       False   \n",
       "13       Business-2460       Location-4ea1         1  4000.0       False   \n",
       "14       Business-aab8       Location-128a         1   815.0       False   \n",
       "15       Business-aab8       Location-128a         1  2500.0       False   \n",
       "16       Business-aab8       Location-128a         1   750.0       False   \n",
       "17       Business-f13b       Location-bb69         1  2255.0       False   \n",
       "18       Business-f13b       Location-bb69         1  2540.0       False   \n",
       "19       Business-f13b       Location-bb69         1  1880.0       False   \n",
       "20       Business-5de0       Location-4ea1         1  2120.0       False   \n",
       "21       Business-5d3e       Location-1979         5  1740.0        True   \n",
       "22       Business-5d3e       Location-1979         5  1695.0        True   \n",
       "23       Business-22a2       Location-c2f2         5  2005.0        True   \n",
       "24       Business-22a2       Location-c2f2         3   640.0       False   \n",
       "\n",
       "        Month-Year  \n",
       "0      August 2024  \n",
       "1      August 2024  \n",
       "2      August 2024  \n",
       "3      August 2024  \n",
       "4      August 2024  \n",
       "5      August 2024  \n",
       "6      August 2024  \n",
       "7      August 2024  \n",
       "8      August 2024  \n",
       "9      August 2024  \n",
       "10     August 2024  \n",
       "11     August 2024  \n",
       "12       June 2024  \n",
       "13       June 2024  \n",
       "14   November 2024  \n",
       "15   November 2024  \n",
       "16   November 2024  \n",
       "17  September 2024  \n",
       "18  September 2024  \n",
       "19  September 2024  \n",
       "20    October 2024  \n",
       "21  September 2024  \n",
       "22  September 2024  \n",
       "23        May 2024  \n",
       "24        May 2024  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Verification of Data Cleaning and Feature Engineering ===\n",
      "\n",
      "Dataset Shape: (329881, 9)\n",
      "\n",
      "Missing Values:\n",
      "No missing values\n",
      "\n",
      "Duplicate Rows: 0\n",
      "\n",
      "Sample of Month-Year feature (first 5 rows):\n",
      "                 DATE   Month-Year\n",
      "0 2024-08-18 21:32:00  August 2024\n",
      "1 2024-08-18 21:32:00  August 2024\n",
      "2 2024-08-18 21:32:00  August 2024\n",
      "3 2024-08-18 21:32:00  August 2024\n",
      "4 2024-08-18 21:32:00  August 2024\n",
      "\n",
      "All Columns in Dataset:\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE', 'is_outlier', 'Month-Year']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Verification of Data Cleaning and Feature Engineering ===\")\n",
    "\n",
    "# 1. Basic Info\n",
    "print(\"\\nDataset Shape:\", df_with_month_year.shape)\n",
    "\n",
    "# 2. Check for missing values\n",
    "missing = df_with_month_year.isnull().sum()\n",
    "print(\"\\nMissing Values:\")\n",
    "print(missing[missing > 0] if missing.any() > 0 else \"No missing values\")\n",
    "\n",
    "# 3. Check for duplicates\n",
    "duplicates = df_with_month_year.duplicated().sum()\n",
    "print(\"\\nDuplicate Rows:\", duplicates)\n",
    "\n",
    "# 4. Show engineered Month-Year feature\n",
    "print(\"\\nSample of Month-Year feature (first 5 rows):\")\n",
    "print(df_with_month_year[['DATE', 'Month-Year']].head())\n",
    "\n",
    "# 5. Show all columns to confirm feature engineering\n",
    "print(\"\\nAll Columns in Dataset:\")\n",
    "print(df_with_month_year.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has been saved successfully!\n",
      "\n",
      "File name: transaction_data_cleaned_and_engineered.csv\n",
      "Number of rows: 329881\n",
      "Number of columns: 9\n",
      "\n",
      "Columns saved:\n",
      "['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION', 'QUANTITY', 'VALUE', 'is_outlier', 'Month-Year']\n"
     ]
    }
   ],
   "source": [
    "# Save \n",
    "df_with_month_year.to_csv('transaction_data_cleaned_and_engineered.csv', index=False)\n",
    "\n",
    "print(\"Dataset has been saved successfully!\")\n",
    "print(f\"\\nFile name: transaction_data_cleaned_and_engineered.csv\")\n",
    "print(f\"Number of rows: {len(df_with_month_year)}\")\n",
    "print(f\"Number of columns: {len(df_with_month_year.columns)}\")\n",
    "print(\"\\nColumns saved:\")\n",
    "print(df_with_month_year.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
